{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6cc5e-f019-417b-83ef-c1da772ea8fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Deriving Evaluation Workflows\n",
    "\n",
    "Objective:\n",
    "\n",
    "> Learning how evaluation workflows are derived from the main ML pipeline.\n",
    "\n",
    "Principles:\n",
    "\n",
    "1. ML metric is a measure to assess the performance of an ML solution.\n",
    "2. *Development evaluation* (of the logical model - a.k.a. the solution) is derived from both the _train mode_ and _apply mode_ workflows combined according to a selected *backtesting method*.\n",
    "3. The workflow for evaluating an already trained (physical) model is composed simply from the model predictions and the (eventually observed) true outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64846413-8b83-4ad5-93b4-ef83bdad0a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Development Evaluation\n",
    "* Essential feedback during the model development process defined as a **function of true and predicted outcomes**.\n",
    "* Indicating a relative change in the solution quality induced by the particular change in its implementation (code).\n",
    "* Working with historical data with known outcomes arranged using a particular **evaluation method**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987ede3-787b-4a08-96ec-7fe8487ff503",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Holdout Method\n",
    "\n",
    "Evaluation method based on part of a training dataset being withheld for testing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1785aa-2bae-41af-92d1-07399ca7a5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from forml import evaluation, project\n",
    "\n",
    "EVALUATION = project.Evaluation(\n",
    "    evaluation.Function(metrics.log_loss), # LogLoss metric function\n",
    "    evaluation.HoldOut(                    # HoldOut evaluation method\n",
    "        test_size=0.2, stratify=True, random_state=42\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549c285-ec78-449e-92b6-a7f4183d18e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Based on the known `SOURCE` and `PIPELINE` components, ForML can produce a task graph to evaluate that solution using the provided definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16a755b-9812-4992-be68-49a826f77495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from forml.pipeline import payload, wrap\n",
    "from dummycatalog import Foo\n",
    "with wrap.importer():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "SOURCE = project.Source.query(Foo.select(Foo.Value), Foo.Label)\n",
    "PIPELINE = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09102c-6902-427f-ac76-fe045fdf2e09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "clear"
    ]
   },
   "outputs": [],
   "source": [
    "SOURCE.bind(PIPELINE, evaluation=EVALUATION).launcher(\n",
    "    runner=\"graphviz\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363cf24-dcc9-4b34-928f-effdc725c6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cross-validation Method\n",
    "Evaluation method based on a number of independent train-test trials using different parts of the same training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01080ace-ff31-4b6a-968a-b3ebfd24cb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "EVALUATION = project.Evaluation(\n",
    "    evaluation.Function(metrics.log_loss),  # LogLoss metric function\n",
    "    evaluation.CrossVal(                    # CrossValidation method\n",
    "        crossvalidator=model_selection.StratifiedKFold(\n",
    "            n_splits=3, shuffle=True, random_state=42\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995b9ea-a250-4aa4-986a-c7c5828af9d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "clear"
    ]
   },
   "outputs": [],
   "source": [
    "SOURCE.bind(PIPELINE, evaluation=EVALUATION).launcher(\n",
    "    runner=\"graphviz\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb8acc-55ff-4a3d-bd3a-c4510a905c24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Production Performance Tracking\n",
    "\n",
    "* Evaluating performance of a physical (trained) model typically making true _future predictions_.\n",
    "* Critical for operational monitoring.\n",
    "* The concept of evaluation methods doesn't apply here.\n",
    "* Depends on a presence of a _feedback loop_ eventually delivering the eventual true outcomes.\n",
    "\n",
    "This will be demonstrated later in scope of the final solution of [Avazu CTR Prediction](../3-solution)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
